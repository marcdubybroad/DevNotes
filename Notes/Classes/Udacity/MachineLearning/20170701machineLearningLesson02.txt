
Lesson 02: Naive Bayes

20170701 - lesson 02, video 1
- classification from supervised learning
  - take features and turn them into labels

- scatter plots
  - M classification algorithm take data and transform into a decision surface
  - decision surface will seperate 2 classes of points
    - can be linear

- Python code
  - sklearn library
    - ml libarys
    - use sklearn Gaussian naive bayes
    
- fitting
  - always train and tets on seperate sets of data
    - if data overlaps, you can overfit to your data
    - so always save 10% of your data to use as your testing set
      - use the test result to report progress of your ML
      
20191225 - wednesday study, video 23
- bayes rule
  - rev thomas bayes
  - cancer exampe
    - P(c) = 0.01 probability of cancer
    - test:
      - 90% correct if positive and you have cancer
        - called sensitivity
      - 90% correct if negative and do not have cancer
        - called specitivity
    - question: if your test is positive, what is the probability that you have cancer
  - explained
    - prior: prob before you run a test
    - then get test evidence when you run the test
    - result in posterior prob
    - so bayes incorporates some information from a test into your prior to arrive to your posterior
    - in cancer
      - prior is cancer chance
      - posterior is prob of cancer given positive test
      - so values are:
        - P(c) = 0.01 - probability of having cancer
        - P(!c) = 0.99 - prob of no cancer
        - P(positive test | c) = 0.9 (prob of positive result given have cancer - sentivity)
        - P(!positive | !c) = 0.9 (prob of negative result if no cancer - specitivity)
        - P(pos | !c) = 1 - P(!pos | !c) = 0.1
        
      - question - if you pos, what chances you have cancer
      - P(have cancer | positive test) = P(c) x P(positive | have cancer)
        - P(c | pos) = 0.01 x 0.9 = 0.009
      - P(no cancer | positive test) = P(!c) x (positive | no cancer)
        - P(!c | pos) = P(!c) x P(pos | !c) = 0.99 x 0.1 = 0.099
    - normalize
      - need both probs to add up to 1; grow them keeping ratio consistent
      - P(c | pos) + P(!c | pos) = 0.009 + 0.099 = 0.108
      - P(pos) = P(c | pos) + P(!c | pos) = 0.108 -> should be 1, so normalize
        - P(c | pos) = 0.009 / 0.108 = 0.0833
        - P(!c | pos) = 0.099 / 0.108 = 0.9167
          - the two above should add up to 1
            - so need to normalize by adding and making sure that end up being 1 (total of test positive, since a given)
          
- video 31
  - using naive bayes for text learning
    - example of 2 peope, chris and sarah using mix of 3 words (love, deal, life)
      - chris is (.1, .8. .1) and sarah (.5, .2, .3)
      - if receive email "love life", and it is 50% prob of either sending an email, who sent it
      - P(chris, "life deal) = .8 * .1 * .5 = .04, P(sarah, "life deal) = .2 * .3 * .5 = .03
        - P(chros | "life deal") = .04 / (.04 + .03) = .57, P(sarah | "life deal") = .03 / (.04 + .03) = .43
      - P(chris, "love deal) = .1 * .8 * .5 = .04, P(sarah, "love deal) = .5 * .2 * .5 = .05
        - P(chros | "love deal") = .04 / (.04 + .05) = .45, P(sarah | "love deal") = .05 / (.04 + .05) = .55
      
- video 35
  - naive bayes
    - lables are hidden, but get to see what words are used, but with different probabilites
      - get evidence for every word, so multiply all evidencs and prior
    - calls naive because it ignores one thing (word order)
  
- video 36
  - NB is easy to implement, works well with huge feature spaces (20k words in engligsh language), and simple/efficient to run
  - breaks in funny ways (doesn't necessarily recognize words that come together as oen meaning, like Chacago Bulls)
    
    
      
    